{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_sentiment_analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVrSYaGuF3YYmq0NKSaZXv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankaj18/NLP-Projects/blob/master/twitter_sentiment_nltk/twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXv-D9MXf8Ns",
        "colab_type": "text"
      },
      "source": [
        "https://monkeylearn.com/sentiment_analysis\n",
        "https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlLLGzDVlriu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4352520f-3af1-45bd-d954-61cb12e98b20"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "plt.style.use('dark_background')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNYVKRnwnSAs",
        "colab_type": "text"
      },
      "source": [
        "download sample tweets from nltk package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLuwjaTJmOBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5b2d42f4-213f-4839-fc11-e9440776ebbb"
      },
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq50InC3mcM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HVG3f2Imxds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tweets=twitter_samples.strings('positive_tweets.json') #5000 tweets with pos sentiment\n",
        "negative_tweets=twitter_samples.strings('negative_tweets.json')#5000 tweets with neg sentiment\n",
        "text=twitter_samples.strings('tweets.20150430-223406.json')#20000 tweets with no sentiment"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJrnFGvdoV7J",
        "colab_type": "text"
      },
      "source": [
        "Strings() method of twitter_samples will print all of the tweets within dataest as strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_wlN4egozAP",
        "colab_type": "text"
      },
      "source": [
        "punkt module is pretrained model that helps for tokenize words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULWpGRoioRwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_tokens=twitter_samples.tokenized('positive_tweets.json')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8tl-9r8d36B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "532c763b-3237-4f8b-f5be-ff11eebbc643"
      },
      "source": [
        "print(tweet_tokens[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfEpa-Pdd86O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a19824bc-f87c-4d89-fe4b-da29da548533"
      },
      "source": [
        "print(pos_tag(tweet_tokens[0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX5j15ZJeJpK",
        "colab_type": "text"
      },
      "source": [
        "#Normalizing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csLG_Lv2fmAO",
        "colab_type": "text"
      },
      "source": [
        "bring data to base form. the function lemantize_srntence first get position token of a tweet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKXEIdM7eL49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize_sentence(tokens):\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  lemmatized_sentence=[]\n",
        "  for word,tag in pos_tag(tokens):\n",
        "    print(word,tag)\n",
        "    #print(tag)\n",
        "    if tag.startswith('NN'):\n",
        "      pos='n'\n",
        "    elif tag.startswith('VB'):\n",
        "      pos='v'\n",
        "    else:\n",
        "      pos='a'\n",
        "    lemmatized_sentence.append(lemmatizer.lemmatize(word,pos))\n",
        "  return lemmatized_sentence"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Yg9UGdknri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "ccc6fb11-0bea-4944-ce70-f07b570ef0cc"
      },
      "source": [
        "print(lemmatize_sentence(tweet_tokens[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#FollowFriday JJ\n",
            "@France_Inte NNP\n",
            "@PKuchly57 NNP\n",
            "@Milipol_Paris NNP\n",
            "for IN\n",
            "being VBG\n",
            "top JJ\n",
            "engaged VBN\n",
            "members NNS\n",
            "in IN\n",
            "my PRP$\n",
            "community NN\n",
            "this DT\n",
            "week NN\n",
            ":) NN\n",
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q91s54xrvKSE",
        "colab_type": "text"
      },
      "source": [
        "word being changes to be and members changes to member"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LenxC4lBkurl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "09cc7670-cdaf-4a9a-f96d-6740911d4778"
      },
      "source": [
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('being','v')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'be'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWYgdqQ-vr51",
        "colab_type": "text"
      },
      "source": [
        "Removing noise from data\n",
        "* Hyperlinks\n",
        "* Twitter handles\n",
        "* Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wu6EsIZow__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string=\" Hi i am pankaj. my email id is pankaj.eln@gmail.com. @hawa. i wnt to check website http://google.com and https://google.co.in\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtex-V1-yVYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern1=\"http:\\S+\"\n",
        "pattern2=\"https:\\S+\"\n",
        "pattern3=\"@[a-zA-Z0-9]+\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOf3K4JNiLDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS35WOGsijT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "7183f9e0-8608-4e78-f016-1d55de806bca"
      },
      "source": [
        "word_tokenize(string)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'i',\n",
              " 'am',\n",
              " 'pankaj',\n",
              " '.',\n",
              " 'my',\n",
              " 'email',\n",
              " 'id',\n",
              " 'is',\n",
              " 'pankaj.eln',\n",
              " '@',\n",
              " 'gmail.com',\n",
              " '.',\n",
              " '@',\n",
              " 'hawa',\n",
              " '.',\n",
              " 'i',\n",
              " 'wnt',\n",
              " 'to',\n",
              " 'check',\n",
              " 'website',\n",
              " 'http',\n",
              " ':',\n",
              " '//google.com',\n",
              " 'and',\n",
              " 'https',\n",
              " ':',\n",
              " '//google.co.in']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sexF_J9fzlq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cb4926fc-458e-42fb-b234-621c5e1d72ff"
      },
      "source": [
        "s1=re.sub(pattern1,\"\",string)\n",
        "s1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hi i am pankaj. my email id is pankaj.eln@gmail.com. @hawa. i wnt to check website  and https://google.co.in'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjalUKJyz5Bv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c01c02fc-b527-44b0-9486-8a59130538a9"
      },
      "source": [
        "s2=re.sub(pattern2,\" \",s1)\n",
        "s2"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hi i am pankaj. my email id is pankaj.eln@gmail.com. @hawa. i wnt to check website  and  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nQc945l0oVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f295746b-21f4-4e50-e90a-8a2c9a7da3e1"
      },
      "source": [
        "s3=re.sub(pattern3,\" \",s2)\n",
        "s3"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hi i am pankaj. my email id is pankaj.eln .com.  . i wnt to check website  and  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYtqVgFfDa4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd3mGh8Y0sUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_noise(tweet_tokens,stop_words=()):\n",
        "  cleaned_tokens=[]\n",
        "  for token,tag in pos_tag(tweet_tokens):\n",
        "    token=re.sub('http:\\S+',\"\",token)\n",
        "    token=re.sub('https:\\S+',\"\",token)\n",
        "    token=re.sub('@[a-zA-Z0-9_]+',\"\",token)\n",
        "    if tag.startswith(\"NN\"):\n",
        "      pos=\"n\"\n",
        "    elif tag.startswith(\"VB\"):\n",
        "      pos=\"v\"\n",
        "    else:\n",
        "      pos=\"a\"\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    token=lemmatizer.lemmatize(token,pos)\n",
        "    if ((len(token)>0) and (token not in string.punctuation) and (token.lower() not in stop_words)):\n",
        "      cleaned_tokens.append(token.lower())\n",
        "  return cleaned_tokens  \n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mszit7QJiUdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=stopwords.words('english')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm9snm9tlYyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "defcd0e4-d8d5-4f40-fa75-f22d725f9ccf"
      },
      "source": [
        "for i in range(10):\n",
        "  print(remove_noise(tweet_tokens[i],stop_words))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n",
            "['hey', 'james', 'odd', ':/', 'please', 'call', 'contact', 'centre', '02392441234', 'able', 'assist', ':)', 'many', 'thanks']\n",
            "['listen', 'last', 'night', ':)', 'bleed', 'amazing', 'track', 'scotland']\n",
            "['congrats', ':)']\n",
            "['yeaaaah', 'yippppy', 'accnt', 'verify', 'rqst', 'succeed', 'get', 'blue', 'tick', 'mark', 'fb', 'profile', ':)', '15', 'day']\n",
            "['one', 'irresistible', ':)', '#flipkartfashionfriday']\n",
            "['like', 'keep', 'lovely', 'customer', 'wait', 'long', 'hope', 'enjoy', 'happy', 'friday', 'lwwf', ':)']\n",
            "['second', 'thought', '’', 'enough', 'time', 'dd', ':)', 'new', 'short', 'enter', 'system', 'sheep', 'must', 'buy']\n",
            "['jgh', 'go', 'bayan', ':d', 'bye']\n",
            "['act', 'mischievousness', 'call', 'etl', 'layer', 'in-house', 'warehouse', 'app', 'katamari', 'well', '…', 'name', 'imply', ':p']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq7daLjpl336",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "40c952c6-65ec-4402-d828-79308b7d6653"
      },
      "source": [
        "tweet_tokens[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday',\n",
              " '@France_Inte',\n",
              " '@PKuchly57',\n",
              " '@Milipol_Paris',\n",
              " 'for',\n",
              " 'being',\n",
              " 'top',\n",
              " 'engaged',\n",
              " 'members',\n",
              " 'in',\n",
              " 'my',\n",
              " 'community',\n",
              " 'this',\n",
              " 'week',\n",
              " ':)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvoXhzfEpUAX",
        "colab_type": "text"
      },
      "source": [
        "use remove_noise function to clean positive and negative tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya_TA135okJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_tweet_tokens=twitter_samples.tokenized('positive_tweets.json')\n",
        "neg_tweet_tokens=twitter_samples.tokenized('negative_tweets.json')\n",
        "positive_cleaned_token_list=[]\n",
        "negative_cleaned_token_list=[]\n",
        "for tokens in pos_tweet_tokens:\n",
        "  positive_cleaned_token_list.append(remove_noise(tokens,stop_words))\n",
        "for tokena in neg_tweet_tokens:\n",
        "  negative_cleaned_token_list.append(remove_noise(tokens,stop_words))\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFyvkO6c9QY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "aaeea5fc-9521-4162-dbe5-6c636cffc913"
      },
      "source": [
        "print(pos_tweet_tokens[500])\n",
        "print(positive_cleaned_token_list[500])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
            "['dang', 'rad', '#fanart', ':d']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhhV19pS9uuB",
        "colab_type": "text"
      },
      "source": [
        "# determine word density"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQq5HvPr-WaH",
        "colab_type": "text"
      },
      "source": [
        "analyze word frequency of positive tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSeitA-k9mg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "  for tokens in cleaned_tokens_list:\n",
        "    for token in tokens:\n",
        "      yield token"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpREtpxI_hHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_pos_words=get_all_words(positive_cleaned_token_list)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZMW8DbFh8G-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "11b2af08-452c-4341-97dc-262afac339de"
      },
      "source": [
        "next(all_pos_words)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#followfriday'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDNOoxLQ_q9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e16f5364-fb9f-439d-9335-d29dd6722da5"
      },
      "source": [
        "freq_dist_pos=FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F1PkyihCXdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53c611dd-44c7-4c29-c9cf-e233d525a69c"
      },
      "source": [
        "positive_cleaned_token_list[0]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0FRUV6PD-yK",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Data for Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LnO6iLEEmta",
        "colab_type": "text"
      },
      "source": [
        "## Converting tokens into dictionary\n",
        "To implement Naive bayes classifier model. this requires python dictionary with word as keys and True as values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtiuyTn8C9Zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "  for tweet_token in cleaned_tokens_list:\n",
        "    yield dict([token, True] for token in tweet_token)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI_uhtwalASP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tokens_for_model=get_tweets_for_model(positive_cleaned_token_list)\n",
        "negative_tokens_for_model=get_tweets_for_model(negative_cleaned_token_list)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNbaYvO1ma_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9cae34f4-0ad2-47f5-cd81-aefdce2cba12"
      },
      "source": [
        "next(positive_tokens_for_model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'#followfriday': True,\n",
              " ':)': True,\n",
              " 'community': True,\n",
              " 'engage': True,\n",
              " 'member': True,\n",
              " 'top': True,\n",
              " 'week': True}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDrZpAgplVQ5",
        "colab_type": "text"
      },
      "source": [
        "Splitting Dataset for training and testing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7rZJwKQlOOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "positive_dataset=[(tweet_dict,\"Positive\") for tweet_dict in positive_tokens_for_model]\n",
        "negative_dataset=[(tweet_dict,\"Negative\") for tweet_dict in negative_tokens_for_model]\n",
        "dataset=positive_dataset+negative_dataset\n",
        "random.shuffle(dataset)\n",
        "train_data=dataset[:7000]\n",
        "test_data=dataset[7000:]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7XD3hh3mOHL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "66127d3a-8471-451a-fb4d-c29b01722779"
      },
      "source": [
        "# this code attaches positive and negative labels to each tweet.\n",
        "# it creates dataset by joining positive and negative tweets.\n",
        "# divide dataset into 70:30 ratio\n",
        "train_data[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({':-)': True, 'could': True, 'egg': True, 'face': True, 'say': True},\n",
              " 'Negative')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BqH_C-gosBt",
        "colab_type": "text"
      },
      "source": [
        "# Building and Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcGP1lJInyKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "013ab88d-7f10-4644-92d2-99ae95d04171"
      },
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier=NaiveBayesClassifier.train(train_data)\n",
        "print(\"accuracy is:\",classify.accuracy(classifier,test_data))\n",
        "print(classifier.show_most_informative_features(20))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is: 0.995998666222074\n",
            "Most Informative Features\n",
            "                     egg = True           Negati : Positi =   1402.6 : 1.0\n",
            "                    face = True           Negati : Positi =    369.1 : 1.0\n",
            "                   could = True           Negati : Positi =    149.2 : 1.0\n",
            "                     say = True           Negati : Positi =     64.3 : 1.0\n",
            "                     :-) = True           Negati : Positi =      7.6 : 1.0\n",
            "                      :) = None           Negati : Positi =      3.5 : 1.0\n",
            "                      :d = None           Negati : Positi =      1.1 : 1.0\n",
            "                  thanks = None           Negati : Positi =      1.1 : 1.0\n",
            "                  follow = None           Negati : Positi =      1.1 : 1.0\n",
            "                    love = None           Negati : Positi =      1.1 : 1.0\n",
            "                     ... = None           Negati : Positi =      1.1 : 1.0\n",
            "                    good = None           Negati : Positi =      1.1 : 1.0\n",
            "                     get = None           Negati : Positi =      1.1 : 1.0\n",
            "                   thank = None           Negati : Positi =      1.1 : 1.0\n",
            "                     day = None           Negati : Positi =      1.1 : 1.0\n",
            "                    like = None           Negati : Positi =      1.0 : 1.0\n",
            "                       u = None           Negati : Positi =      1.0 : 1.0\n",
            "                   happy = None           Negati : Positi =      1.0 : 1.0\n",
            "                      hi = None           Negati : Positi =      1.0 : 1.0\n",
            "                    know = None           Negati : Positi =      1.0 : 1.0\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUMYsYwg9uoA",
        "colab_type": "text"
      },
      "source": [
        "# Testing Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2CRQnYE6qem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e9a76f9-42f6-42b4-f396-80d342c61ccc"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "custom_tweet=\"I ordered just one from flipkart,they screwed up, never used the app again\"\n",
        "custom_tokens=remove_noise(word_tokenize(custom_tweet))\n",
        "print(classifier.classify(dict([token,True] for token in custom_tokens)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFW-59sK-cQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}