{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Keras LSTM IMDB sentiment classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankaj18/NLP-Projects/blob/master/nlp_preprocessing/Keras_LSTM_IMDB_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFVMFealsIdM"
      },
      "source": [
        "Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "\n",
        "#### Notes\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9pGcYMLsIde",
        "outputId": "9ead400a-f023-4ee4-cab1-7f96d0baaba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Import libraries\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb #in-built imdb dataset available in keras\n",
        "\n",
        "max_features = 20000\n",
        "maxlength = 80  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 36\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) #loading dataset here\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlength)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlength)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=20,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Build model...\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Train...\n",
            "Epoch 1/20\n",
            "695/695 [==============================] - 222s 312ms/step - loss: 0.5367 - accuracy: 0.7089 - val_loss: 0.3529 - val_accuracy: 0.8437\n",
            "Epoch 2/20\n",
            "695/695 [==============================] - 215s 310ms/step - loss: 0.2488 - accuracy: 0.9017 - val_loss: 0.4732 - val_accuracy: 0.8150\n",
            "Epoch 3/20\n",
            "695/695 [==============================] - 214s 308ms/step - loss: 0.1636 - accuracy: 0.9387 - val_loss: 0.4381 - val_accuracy: 0.8294\n",
            "Epoch 4/20\n",
            "695/695 [==============================] - 215s 309ms/step - loss: 0.0959 - accuracy: 0.9667 - val_loss: 0.6071 - val_accuracy: 0.8106\n",
            "Epoch 5/20\n",
            "695/695 [==============================] - 214s 308ms/step - loss: 0.0672 - accuracy: 0.9766 - val_loss: 0.6360 - val_accuracy: 0.8220\n",
            "Epoch 6/20\n",
            "695/695 [==============================] - 213s 307ms/step - loss: 0.0473 - accuracy: 0.9844 - val_loss: 0.6433 - val_accuracy: 0.8174\n",
            "Epoch 7/20\n",
            "695/695 [==============================] - 213s 307ms/step - loss: 0.0374 - accuracy: 0.9882 - val_loss: 0.7062 - val_accuracy: 0.8161\n",
            "Epoch 8/20\n",
            "695/695 [==============================] - 213s 306ms/step - loss: 0.0226 - accuracy: 0.9925 - val_loss: 0.9448 - val_accuracy: 0.8158\n",
            "Epoch 9/20\n",
            "695/695 [==============================] - 213s 306ms/step - loss: 0.0202 - accuracy: 0.9936 - val_loss: 0.9523 - val_accuracy: 0.8166\n",
            "Epoch 10/20\n",
            "695/695 [==============================] - 212s 305ms/step - loss: 0.0192 - accuracy: 0.9937 - val_loss: 0.9721 - val_accuracy: 0.8134\n",
            "Epoch 11/20\n",
            "695/695 [==============================] - 213s 307ms/step - loss: 0.0136 - accuracy: 0.9955 - val_loss: 0.9166 - val_accuracy: 0.8124\n",
            "Epoch 12/20\n",
            "695/695 [==============================] - 213s 306ms/step - loss: 0.0103 - accuracy: 0.9976 - val_loss: 1.0582 - val_accuracy: 0.8124\n",
            "Epoch 13/20\n",
            "695/695 [==============================] - 212s 306ms/step - loss: 0.0104 - accuracy: 0.9964 - val_loss: 1.2110 - val_accuracy: 0.8136\n",
            "Epoch 14/20\n",
            "695/695 [==============================] - 212s 305ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 1.1272 - val_accuracy: 0.8198\n",
            "Epoch 15/20\n",
            "695/695 [==============================] - 212s 306ms/step - loss: 0.0082 - accuracy: 0.9981 - val_loss: 1.2446 - val_accuracy: 0.8196\n",
            "Epoch 16/20\n",
            "695/695 [==============================] - 212s 305ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 1.2868 - val_accuracy: 0.8076\n",
            "Epoch 17/20\n",
            "695/695 [==============================] - 212s 305ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 1.1298 - val_accuracy: 0.8123\n",
            "Epoch 18/20\n",
            "695/695 [==============================] - 214s 308ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 1.2175 - val_accuracy: 0.8127\n",
            "Epoch 19/20\n",
            "695/695 [==============================] - 213s 307ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 1.3931 - val_accuracy: 0.8122\n",
            "Epoch 20/20\n",
            "695/695 [==============================] - 212s 305ms/step - loss: 0.0065 - accuracy: 0.9976 - val_loss: 1.4916 - val_accuracy: 0.8038\n",
            "695/695 [==============================] - 13s 19ms/step - loss: 1.4916 - accuracy: 0.8038\n",
            "Test score: 1.4915651082992554\n",
            "Test accuracy: 0.8037599921226501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Vz25lIsIdl"
      },
      "source": [
        "Explain the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B9tgx7WsIdn",
        "outputId": "2af99c93-b1f3-4591-c117-1cb1ad36b6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "!pip install shap\n",
        "import shap\n",
        "\n",
        "# we are taking first 150 training dataset as our background dataset to integerate over it\n",
        "explainer = shap.DeepExplainer(model, x_train[:150])\n",
        "\n",
        "# explain the first 20 predictions\n",
        "# explaining each prediction requires 2 * background dataset size runs\n",
        "shap_values = explainer.shap_values(x_test[:20])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f4/c5b95cddae15be80f8e58b25edceca105aa83c0b8c86a1edad24a6af80d3/shap-0.39.0.tar.gz (356kB)\n",
            "\r\u001b[K     |█                               | 10kB 25.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 19.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 11.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 143kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 163kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 174kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 184kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 194kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 204kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 215kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 225kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 235kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 245kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 276kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 286kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 296kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 307kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 317kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 327kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 337kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 348kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.41.1)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (54.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491641 sha256=3398a4028d36bfe50cb69cd2a9c2334c04e2285f1c37f450cba99196520f3e11\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/27/f5/a8ab9da52fd159aae6477b5ede6eaaec69fd130fa0fa59f283\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.39.0 slicer-0.0.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "keras is no longer supported, please use tf.keras instead.\n",
            "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode. See PR #1483 for discussion.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2f5fb3d65426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# explain the first 20 predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# explaining each prediction requires 2 * background dataset size runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/shap/explainers/_deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;31m# run attribution computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                 \u001b[0msample_phis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi_symbolic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_with_overridden_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mexecute_with_overridden_gradients\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;31m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# reinstate the backpropagatable check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36manon\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m                     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0OjJkecsIdp"
      },
      "source": [
        "# init JS visualization code\n",
        "shap.initjs()\n",
        "\n",
        "# index to words transforming\n",
        "import numpy as np\n",
        "words = imdb.get_word_index()\n",
        "num2word = {}\n",
        "for w in words.keys():\n",
        "    num2word[words[w]] = w\n",
        "x_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), x_test[i]))) for i in range(10)])\n",
        "\n",
        "# plot the explanation of the first prediction\n",
        "# Note the model is \"multi-output\" because it is rank-2 but only has one column\n",
        "shap.force_plot(explainer.expected_value[0], shap_values[0][0], x_test_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBQKgwVWsIdt"
      },
      "source": [
        "Here, We note that each sample is an IMDB review text document, represented as the sequence of words. This means that \"feature 0\" is the first word in the review, which will be different for difference reviews. This means calling summary_plot will combine the importance of all the words by their position in the text. This is likely not what you want for a global measure of feature importance (which is why we have not called summary_plot here). If you do want a global summary of a word's importance you could pull apart the feature attribution values and group them by words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUskO7gRsIdv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}