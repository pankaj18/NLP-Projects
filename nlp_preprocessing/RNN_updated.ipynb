{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "RNN_updated.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankaj18/NLP-Projects/blob/master/nlp_preprocessing/RNN_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zzRkOezsFrI"
      },
      "source": [
        "# RNN (Recurrent Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQFngST7sFrZ"
      },
      "source": [
        "## 1)  After CNN, why RNN is there?\n",
        "\n",
        "The backpropagation algorithm in CNN (convolutional neural network), we know that their output only considers the influence of the previous input and does not consider the influence of other moments of input, such as simple cats, dogs, handwritten numbers and other single objects.\n",
        "\n",
        "However, for some related to time, such as the prediction of the next moment of the video, the prediction of the content of the previous and subsequent documents, etc., the performance of these algorithms is not satisfactory. Therefore, RNN should be applied and was born."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFhiSZvSsFrb"
      },
      "source": [
        "## 2) What is RNN?\n",
        "\n",
        "RNN is a special neural network structure, which is proposed based on the view that \" human cognition is based on past experience and memory \". It is different from DNN and CNN in that it not only considers the input of the previous moment, And it gives the network a 'memory' function of the previous content .\n",
        "\n",
        "The reason why RNN is called recurrent neural network is that the current output of a sequence is also related to the previous output. The specific manifestation is that the network memorizes the previous information and applies it to the current output calculation, that is, the nodes between the hidden layers are connected, and the input of the hidden layer includes not only the output of the input layer It also includes the output of the hidden layer from the previous moment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVH0JU1XsFre"
      },
      "source": [
        "## 3) What are the main application areas of RNN ?\n",
        "\n",
        "There are many application fields of RNN. It can be said that as long as the problem of chronological order is considered, RNN can be used to solve it. Here are some common application fields:\n",
        "\n",
        "    ① Natural Language Processing (NLP) : There are video processing ,  text generation , language model , image processing\n",
        "\n",
        "    ② Machine translation , machine writing novels\n",
        "\n",
        "    ③ Speech recognition\n",
        "\n",
        "    ④ Image description generation\n",
        "\n",
        "    ⑤ Text similarity calculation\n",
        "\n",
        "    ⑥ New application areas such as music recommendation , Netease koala product recommendation , Youtube video recommendation, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KofuGvTosFrg"
      },
      "source": [
        "### Different types of RNN\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/0*1PKOwfxLIg_64TAO.jpeg)\n",
        "\n",
        "#### One-to-one:\n",
        "\n",
        "This also called as Plain/Vaniall Neural networks. It deals with Fixed size of input to Fixed size of Output where they are independent of previous information/output.\n",
        "\n",
        "Ex: Image classification.\n",
        "\n",
        "\n",
        "#### One-to-Many:\n",
        "\n",
        "it deals with fixed size of information as input that gives sequence of data as output.\n",
        "\n",
        "Ex:Image Captioning takes image as input and outputs a sentence of words.\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/0*d9FisCKzVZ29SxUu.png)\n",
        "\n",
        "#### Many-to-One:\n",
        "\n",
        "It takes Sequence of information as input and ouputs a fixed size of output.\n",
        "\n",
        "Ex:sentiment analysis where a given sentence is classified as expressing positive or negative sentiment.\n",
        "\n",
        "\n",
        "#### Many-to-Many:\n",
        "\n",
        "It takes a Sequence of information as input and process it recurrently outputs a Sequence of data.\n",
        "\n",
        "Ex: Machine Translation, where an RNN reads a sentence in English and then outputs a sentence in French."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYjyB4P6sFri"
      },
      "source": [
        "## RNN model structure\n",
        "\n",
        "Earlier we said that RNN has the function of \"memory\" of time, so how does it realize the so-called \"memory\"?\n",
        "\n",
        "![alt](img/1.png)\n",
        "\n",
        "<center>Figure 1 RNN structure diagram </center>\n",
        "\n",
        "As shown in Figure 1, we can see that the RNN hierarchy is simpler than CNN.It mainly consists of an input layer , a Hidden Layer , and an output layer .\n",
        "\n",
        "And you will find that **there is an arrow in the Hidden Layer to  indicate the cyclic update of the data.This is the method to implement the time memory function.**\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/1*xn5kA92_J5KLaKcP7BMRLA.gif)\n",
        "\n",
        "* t — time step\n",
        "* X — input\n",
        "* h — hidden state\n",
        "* length of X — size/dimension of input\n",
        "* length of h — no. of hidden units.\n",
        "\n",
        "![alt](img/2.png)\n",
        "\n",
        "<center>Figure 2 Unfolded RNN Diagram </center>\n",
        "\n",
        "Figure 2 shows the hierarchical expansion of the Hidden Layer where\n",
        "\n",
        "**T-1, t, t + 1 represent the time series**\n",
        "\n",
        "**X represents the input sample**\n",
        "\n",
        "**St represents the memory of the sample at time t, St = f (W * St -1 + U * Xt).** \n",
        "\n",
        "**W is the weight of the input**\n",
        "\n",
        "**U is the weight of the input sample at the moment**\n",
        "\n",
        "**V is the weight of the output sample.**\n",
        "\n",
        "\n",
        "### Feedforward\n",
        "\n",
        "At t = 1, the general initialization input S0 = 0, randomly initializes W, U, V, and calculates the following formula:\n",
        "\n",
        "![alt](img/21.png)\n",
        "\n",
        "Among them, f and g are activation functions. Among them, f can be tanh, relu, sigmoid and other activation functions, g is usually softmax or other.\n",
        "\n",
        "Time advances, and the state s1 at this time, as the memory state at time 1, will participate in the prediction activity at the next time, that is:\n",
        "\n",
        "![alt](img/22.png)\n",
        "\n",
        "By analogy, you can get the final output value:\n",
        "\n",
        "![alt](img/23.png)\n",
        "\n",
        "\n",
        "Note : \n",
        "\n",
        "1. Here W, U, V are equal at each moment ( weight sharing ).\n",
        "\n",
        "2. The hidden state can be understood as: S = f (existing input + past memory summary )\n",
        "\n",
        "\n",
        "## Back propagation through TIME of RNN\n",
        "\n",
        "Earlier we introduced the forward propagation method of RNN, so how are the weight parameters W, U, and V of the RNN updated?\n",
        "\n",
        "Each output will have a value Ot error value , Et the total error may be expressed as: ![alt](img/31.png)\n",
        "\n",
        " The loss function can use either the cross-entropy loss function or the squared error loss function .\n",
        "\n",
        "Because the output of each step does not only depend on the network of the current step, but also the state of the previous steps, then this modified BP algorithm is called Backpropagation Through Time ( BPTT ), which is the reverse transfer of the error value at the output end. The gradient descent method is updated. \n",
        "\n",
        "That is, the gradient of the parameter is required:\n",
        "\n",
        "![alt](img/32.png)\n",
        "\n",
        " First we solve the update method of W. From the previous update of W, it can be seen that it is the sum of the partial derivatives of the deviations at each moment. \n",
        "\n",
        "Here we take time t = 3 as an example.According to the chain derivation rule, we can get the partial derivative at time t = 3 as:\n",
        "\n",
        "\n",
        "![alt](img/331.png)\n",
        "\n",
        "\n",
        "At this time, according to the formula, ![alt](img/331.png) we will find that in addition to W, S3 is also related to S2 at the previous moment.\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/0*ENwCVS8XI8cjCy55.jpg)\n",
        "\n",
        "For S3, expand directly to get the following formula:\n",
        "\n",
        "![alt](img/34.png)\n",
        "\n",
        "For S2, expand directly to get the following formula:\n",
        "\n",
        "![alt](img/35.png)\n",
        "\n",
        "For S1, expand directly to get the following formula:\n",
        "\n",
        "![alt](img/36.png)\n",
        "\n",
        "Combine the above three formulas to get:\n",
        "\n",
        "![alt](img/37.png)\n",
        "\n",
        "This gives the formula:\n",
        "\n",
        "![alt](img/381.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXZAiaAcsFrn"
      },
      "source": [
        "What is to be explained here is ![alt](img/39.png)  that it means that S3 directly differentiates W without considering the effect of S2 (that is, for example, y = f (x) * g (x) is the same as the derivative of x)\n",
        "\n",
        "The second is the update method for U. Since the parameter U and W are similar, they will not be described here, and the specific formula finally obtained is as follows:\n",
        "\n",
        "![alt](img/40.png)\n",
        "\n",
        "Finally, give the updated formula of V (V is only related to output O):\n",
        "\n",
        "![alt](img/41.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSoa3YcNsFro"
      },
      "source": [
        "### Going little deeper\n",
        "\n",
        "Let’s focus on one error term et.\n",
        "\n",
        "You’ve calculated the cost function et, and now you want to propagate your cost function back through the network because you need to update the weights.\n",
        "\n",
        "Essentially, every single neuron that participated in the calculation of the output, associated with this cost function, should have its weight updated in order to minimize that error. And the thing with RNNs is that it’s not just the neurons directly below this output layer that contributed but all of the neurons far back in time. So, you have to propagate all the way back through time to these neurons.\n",
        "\n",
        "The problem relates to updating wrec (weight recurring) – the weight that is used to connect the hidden layers to themselves in the unrolled temporal loop.\n",
        "\n",
        "For instance, to get from xt-3 to xt-2 we multiply xt-3 by wrec. Then, to get from xt-2 to xt-1 we again multiply xt-2 by wrec. So, we multiply with the same exact weight multiple times, and this is where the problem arises: when you multiply something by a small number, your value decreases very quickly.\n",
        "\n",
        "As we know, weights are assigned at the start of the neural network with the random values, which are close to zero, and from there the network trains them up. But, when you start with wrec close to zero and multiply xt, xt-1, xt-2, xt-3, … by this value, your gradient becomes less and less with each multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUl-54yDsFrp"
      },
      "source": [
        "#### What does this mean for the network?\n",
        "\n",
        "The lower the gradient is, the harder it is for the network to update the weights and the longer it takes to get to the final result.\n",
        "\n",
        "For instance, 1000 epochs might be enough to get the final weight for the time point t, but insufficient for training the weights for the time point t-3 due to a very low gradient at this point. However, the problem is not only that half of the network is not trained properly.\n",
        "\n",
        "The output of the earlier layers is used as the input for the further layers. Thus, the training for the time point t is happening all along based on inputs that are coming from untrained layers. So, because of the vanishing gradient, the whole network is not being trained properly.\n",
        "\n",
        "To sum up, if wrec is small, you have vanishing gradient problem, and if wrec is large, you have exploding gradient problem.\n",
        "\n",
        "For the vanishing gradient problem, the further you go through the network, the lower your gradient is and the harder it is to train the weights, which has a domino effect on all of the further weights throughout the network.\n",
        "\n",
        "That was the main roadblock to using Recurrent Neural Networks. But let’s now check what are the possible solutions to this problem.\n",
        "\n",
        "Solutions to the vanishing gradient problem\n",
        "In case of exploding gradient, you can:\n",
        "\n",
        "*    stop backpropagating after a certain point, which is usually not optimal because not all of the weights get updated;\n",
        "*    penalize or artificially reduce gradient;\n",
        "*    put a maximum limit on a gradient.\n",
        "\n",
        "\n",
        "In case of vanishing gradient, you can:\n",
        "\n",
        "*    initialize weights so that the potential for vanishing gradient is minimized;\n",
        "*    have Echo State Networks that are designed to solve the vanishing gradient problem;\n",
        "*    have Long Short-Term Memory Networks (LSTMs).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFC9f7PcsFrq"
      },
      "source": [
        "### Advantages of Recurrent Neural Network\n",
        "\n",
        "-    RNN can model sequence of data so that each sample can be assumed to be dependent on previous ones\n",
        "-    Recurrent neural network are even used with convolutional layers to extend the effective pixel neighbourhood.\n",
        "\n",
        "### Disadvantages of Recurrent Neural Network\n",
        "\n",
        "-   Gradient vanishing and exploding problems.\n",
        "-   Training an RNN is a very difficult task.\n",
        "-   It cannot process very long sequences if using tanh or relu as an activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ4kNDDasFrr"
      },
      "source": [
        "# Building Classifers using the Reuters Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMlLYBqAsFrv"
      },
      "source": [
        "## Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFEv49yUsFrw",
        "outputId": "fd511fe2-0316-43bd-fc8f-f0a907434499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# parameters for data load\n",
        "num_words = 30000\n",
        "maxlen = 50\n",
        "test_split = 0.3\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words = num_words, maxlen = maxlen, test_split = test_split)\n",
        "\n",
        "\n",
        "\n",
        "# pad the sequences with zeros \n",
        "# padding parameter is set to 'post' => 0's are appended to end of sequences\n",
        "X_train = pad_sequences(X_train, padding = 'post')\n",
        "X_test = pad_sequences(X_test, padding = 'post')\n",
        "\n",
        "X_train = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "y_data = np.concatenate((y_train, y_test))\n",
        "y_data = to_categorical(y_data)\n",
        "y_train = y_data[:1395]\n",
        "y_test = y_data[1395:]\n",
        "\n",
        "def vanilla_rnn():\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(50, input_shape = (49,1), return_sequences = False))\n",
        "    model.add(Dense(46))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.Adam(lr = 0.001)\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn = vanilla_rnn, epochs = 200, batch_size = 50, verbose = 1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_test_ = np.argmax(y_test, axis = 1)\n",
        "\n",
        "print(accuracy_score(y_pred, y_test_))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "28/28 [==============================] - 1s 9ms/step - loss: 3.5305 - accuracy: 0.1970\n",
            "Epoch 2/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.5684 - accuracy: 0.7096\n",
            "Epoch 3/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.2188 - accuracy: 0.7108\n",
            "Epoch 4/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.2069 - accuracy: 0.6981\n",
            "Epoch 5/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1795 - accuracy: 0.7177\n",
            "Epoch 6/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.2234 - accuracy: 0.7042\n",
            "Epoch 7/200\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 1.1463 - accuracy: 0.7141\n",
            "Epoch 8/200\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 1.1261 - accuracy: 0.7207\n",
            "Epoch 9/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1163 - accuracy: 0.7285\n",
            "Epoch 10/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.1749 - accuracy: 0.7083\n",
            "Epoch 11/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.1322 - accuracy: 0.7172\n",
            "Epoch 12/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1187 - accuracy: 0.7253\n",
            "Epoch 13/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1138 - accuracy: 0.7292\n",
            "Epoch 14/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.1265 - accuracy: 0.7177\n",
            "Epoch 15/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1216 - accuracy: 0.7171\n",
            "Epoch 16/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0761 - accuracy: 0.7304\n",
            "Epoch 17/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1706 - accuracy: 0.7180\n",
            "Epoch 18/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.1233 - accuracy: 0.7146\n",
            "Epoch 19/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1528 - accuracy: 0.7142\n",
            "Epoch 20/200\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 1.1738 - accuracy: 0.7083\n",
            "Epoch 21/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1177 - accuracy: 0.7263\n",
            "Epoch 22/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.1238 - accuracy: 0.7169\n",
            "Epoch 23/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1388 - accuracy: 0.7184\n",
            "Epoch 24/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1235 - accuracy: 0.7089\n",
            "Epoch 25/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1161 - accuracy: 0.7267\n",
            "Epoch 26/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.1164 - accuracy: 0.7138\n",
            "Epoch 27/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0819 - accuracy: 0.7220\n",
            "Epoch 28/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1242 - accuracy: 0.7086\n",
            "Epoch 29/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0379 - accuracy: 0.7199\n",
            "Epoch 30/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.0448 - accuracy: 0.7250\n",
            "Epoch 31/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0798 - accuracy: 0.7120\n",
            "Epoch 32/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0733 - accuracy: 0.6920\n",
            "Epoch 33/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0209 - accuracy: 0.7266\n",
            "Epoch 34/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0931 - accuracy: 0.7086\n",
            "Epoch 35/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.0287 - accuracy: 0.7197\n",
            "Epoch 36/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.0222 - accuracy: 0.7283\n",
            "Epoch 37/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0128 - accuracy: 0.7090\n",
            "Epoch 38/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9881 - accuracy: 0.7184\n",
            "Epoch 39/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9551 - accuracy: 0.7374\n",
            "Epoch 40/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0516 - accuracy: 0.7132\n",
            "Epoch 41/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.0323 - accuracy: 0.7321\n",
            "Epoch 42/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0468 - accuracy: 0.7052\n",
            "Epoch 43/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0529 - accuracy: 0.7218\n",
            "Epoch 44/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.9750 - accuracy: 0.7275\n",
            "Epoch 45/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9845 - accuracy: 0.7151\n",
            "Epoch 46/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9791 - accuracy: 0.7249\n",
            "Epoch 47/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.9664 - accuracy: 0.7256\n",
            "Epoch 48/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.0063 - accuracy: 0.7337\n",
            "Epoch 49/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0239 - accuracy: 0.7269\n",
            "Epoch 50/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.9716 - accuracy: 0.7283\n",
            "Epoch 51/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1744 - accuracy: 0.6803\n",
            "Epoch 52/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0679 - accuracy: 0.7310\n",
            "Epoch 53/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1559 - accuracy: 0.7208\n",
            "Epoch 54/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1.1262 - accuracy: 0.7128\n",
            "Epoch 55/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0248 - accuracy: 0.7160\n",
            "Epoch 56/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0226 - accuracy: 0.7132\n",
            "Epoch 57/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0124 - accuracy: 0.7197\n",
            "Epoch 58/200\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.9563 - accuracy: 0.7429\n",
            "Epoch 59/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0051 - accuracy: 0.7324\n",
            "Epoch 60/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9733 - accuracy: 0.7148\n",
            "Epoch 61/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9557 - accuracy: 0.7307\n",
            "Epoch 62/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0100 - accuracy: 0.7048\n",
            "Epoch 63/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9548 - accuracy: 0.7173\n",
            "Epoch 64/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0204 - accuracy: 0.7207\n",
            "Epoch 65/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9717 - accuracy: 0.7284\n",
            "Epoch 66/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9729 - accuracy: 0.7276\n",
            "Epoch 67/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.9700 - accuracy: 0.7272\n",
            "Epoch 68/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9940 - accuracy: 0.7243\n",
            "Epoch 69/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9675 - accuracy: 0.7390\n",
            "Epoch 70/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9390 - accuracy: 0.7289\n",
            "Epoch 71/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8942 - accuracy: 0.7521\n",
            "Epoch 72/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9871 - accuracy: 0.7189\n",
            "Epoch 73/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9848 - accuracy: 0.7189\n",
            "Epoch 74/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0086 - accuracy: 0.7166\n",
            "Epoch 75/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9843 - accuracy: 0.7110\n",
            "Epoch 76/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9347 - accuracy: 0.7323\n",
            "Epoch 77/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0666 - accuracy: 0.6802\n",
            "Epoch 78/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.2008 - accuracy: 0.7027\n",
            "Epoch 79/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1355 - accuracy: 0.7105\n",
            "Epoch 80/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1275 - accuracy: 0.7129\n",
            "Epoch 81/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0336 - accuracy: 0.7448\n",
            "Epoch 82/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0868 - accuracy: 0.7236\n",
            "Epoch 83/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1034 - accuracy: 0.7254\n",
            "Epoch 84/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1746 - accuracy: 0.7088\n",
            "Epoch 85/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.1358 - accuracy: 0.7067\n",
            "Epoch 86/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0876 - accuracy: 0.7152\n",
            "Epoch 87/200\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 1.1099 - accuracy: 0.7128\n",
            "Epoch 88/200\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 1.1288 - accuracy: 0.7070\n",
            "Epoch 89/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1.0327 - accuracy: 0.7236\n",
            "Epoch 90/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9540 - accuracy: 0.7354\n",
            "Epoch 91/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9697 - accuracy: 0.7345\n",
            "Epoch 92/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9956 - accuracy: 0.7141\n",
            "Epoch 93/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9482 - accuracy: 0.7327\n",
            "Epoch 94/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9249 - accuracy: 0.7429\n",
            "Epoch 95/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9976 - accuracy: 0.7089\n",
            "Epoch 96/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9592 - accuracy: 0.7230\n",
            "Epoch 97/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9068 - accuracy: 0.7332\n",
            "Epoch 98/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.9421 - accuracy: 0.7283\n",
            "Epoch 99/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8857 - accuracy: 0.7397\n",
            "Epoch 100/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9663 - accuracy: 0.7236\n",
            "Epoch 101/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.8637 - accuracy: 0.7514\n",
            "Epoch 102/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.9428 - accuracy: 0.7266\n",
            "Epoch 103/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9725 - accuracy: 0.7252\n",
            "Epoch 104/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9583 - accuracy: 0.7237\n",
            "Epoch 105/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9953 - accuracy: 0.7308\n",
            "Epoch 106/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9823 - accuracy: 0.7236\n",
            "Epoch 107/200\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.8998 - accuracy: 0.7493\n",
            "Epoch 108/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9423 - accuracy: 0.7377\n",
            "Epoch 109/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8591 - accuracy: 0.7505\n",
            "Epoch 110/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8973 - accuracy: 0.7505\n",
            "Epoch 111/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8549 - accuracy: 0.7525\n",
            "Epoch 112/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9141 - accuracy: 0.7408\n",
            "Epoch 113/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8708 - accuracy: 0.7560\n",
            "Epoch 114/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.8607 - accuracy: 0.7591\n",
            "Epoch 115/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9027 - accuracy: 0.7311\n",
            "Epoch 116/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9141 - accuracy: 0.7337\n",
            "Epoch 117/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9072 - accuracy: 0.7268\n",
            "Epoch 118/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8394 - accuracy: 0.7546\n",
            "Epoch 119/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8594 - accuracy: 0.7454\n",
            "Epoch 120/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8357 - accuracy: 0.7409\n",
            "Epoch 121/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8758 - accuracy: 0.7392\n",
            "Epoch 122/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8542 - accuracy: 0.7376\n",
            "Epoch 123/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8484 - accuracy: 0.7422\n",
            "Epoch 124/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8860 - accuracy: 0.7413\n",
            "Epoch 125/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.8938 - accuracy: 0.7282\n",
            "Epoch 126/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8098 - accuracy: 0.7563\n",
            "Epoch 127/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8467 - accuracy: 0.7452\n",
            "Epoch 128/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9398 - accuracy: 0.7308\n",
            "Epoch 129/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8365 - accuracy: 0.7490\n",
            "Epoch 130/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8952 - accuracy: 0.7337\n",
            "Epoch 131/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8934 - accuracy: 0.7322\n",
            "Epoch 132/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8630 - accuracy: 0.7452\n",
            "Epoch 133/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8444 - accuracy: 0.7338\n",
            "Epoch 134/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8027 - accuracy: 0.7559\n",
            "Epoch 135/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8667 - accuracy: 0.7448\n",
            "Epoch 136/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.9041 - accuracy: 0.7319\n",
            "Epoch 137/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8608 - accuracy: 0.7502\n",
            "Epoch 138/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8325 - accuracy: 0.7575\n",
            "Epoch 139/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.8737 - accuracy: 0.7509\n",
            "Epoch 140/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.8467 - accuracy: 0.7562\n",
            "Epoch 141/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8348 - accuracy: 0.7460\n",
            "Epoch 142/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8593 - accuracy: 0.7445\n",
            "Epoch 143/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8431 - accuracy: 0.7403\n",
            "Epoch 144/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8324 - accuracy: 0.7536\n",
            "Epoch 145/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8476 - accuracy: 0.7451\n",
            "Epoch 146/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8682 - accuracy: 0.7267\n",
            "Epoch 147/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8633 - accuracy: 0.7390\n",
            "Epoch 148/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8400 - accuracy: 0.7549\n",
            "Epoch 149/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8443 - accuracy: 0.7601\n",
            "Epoch 150/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8099 - accuracy: 0.7646\n",
            "Epoch 151/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9037 - accuracy: 0.7413\n",
            "Epoch 152/200\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.8499 - accuracy: 0.7452\n",
            "Epoch 153/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8226 - accuracy: 0.7458\n",
            "Epoch 154/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8162 - accuracy: 0.7554\n",
            "Epoch 155/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8751 - accuracy: 0.7383\n",
            "Epoch 156/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8393 - accuracy: 0.7430\n",
            "Epoch 157/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8286 - accuracy: 0.7375\n",
            "Epoch 158/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8374 - accuracy: 0.7487\n",
            "Epoch 159/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8473 - accuracy: 0.7364\n",
            "Epoch 160/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8713 - accuracy: 0.7319\n",
            "Epoch 161/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8318 - accuracy: 0.7479\n",
            "Epoch 162/200\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.8306 - accuracy: 0.7484\n",
            "Epoch 163/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8459 - accuracy: 0.7276\n",
            "Epoch 164/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.8248 - accuracy: 0.7356\n",
            "Epoch 165/200\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 0.8759 - accuracy: 0.7508\n",
            "Epoch 166/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9820 - accuracy: 0.7140\n",
            "Epoch 167/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.9602 - accuracy: 0.7309\n",
            "Epoch 168/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8547 - accuracy: 0.7423\n",
            "Epoch 169/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8030 - accuracy: 0.7471\n",
            "Epoch 170/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8635 - accuracy: 0.7435\n",
            "Epoch 171/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8088 - accuracy: 0.7590\n",
            "Epoch 172/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.7803 - accuracy: 0.7600\n",
            "Epoch 173/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8501 - accuracy: 0.7386\n",
            "Epoch 174/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8245 - accuracy: 0.7422\n",
            "Epoch 175/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8546 - accuracy: 0.7397\n",
            "Epoch 176/200\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.8118 - accuracy: 0.7492\n",
            "Epoch 177/200\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.8150 - accuracy: 0.7510\n",
            "Epoch 178/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.7774 - accuracy: 0.7662\n",
            "Epoch 179/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8495 - accuracy: 0.7396\n",
            "Epoch 180/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8058 - accuracy: 0.7551\n",
            "Epoch 181/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8178 - accuracy: 0.7410\n",
            "Epoch 182/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8319 - accuracy: 0.7518\n",
            "Epoch 183/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.7952 - accuracy: 0.7540\n",
            "Epoch 184/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8447 - accuracy: 0.7425\n",
            "Epoch 185/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8233 - accuracy: 0.7447\n",
            "Epoch 186/200\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.8353 - accuracy: 0.7403\n",
            "Epoch 187/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8225 - accuracy: 0.7560\n",
            "Epoch 188/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8344 - accuracy: 0.7485\n",
            "Epoch 189/200\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.8656 - accuracy: 0.7262\n",
            "Epoch 190/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.7746 - accuracy: 0.7627\n",
            "Epoch 191/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.7598 - accuracy: 0.7728\n",
            "Epoch 192/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8180 - accuracy: 0.7491\n",
            "Epoch 193/200\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.7815 - accuracy: 0.7570\n",
            "Epoch 194/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.7910 - accuracy: 0.7602\n",
            "Epoch 195/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.7739 - accuracy: 0.7524\n",
            "Epoch 196/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8689 - accuracy: 0.7444\n",
            "Epoch 197/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8777 - accuracy: 0.7456\n",
            "Epoch 198/200\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.8220 - accuracy: 0.7504\n",
            "Epoch 199/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.7661 - accuracy: 0.7529\n",
            "Epoch 200/200\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.8055 - accuracy: 0.7585\n",
            " 1/12 [=>............................] - ETA: 1s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 0s 4ms/step\n",
            "0.7662771285475793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P23Mbu4sFr1"
      },
      "source": [
        "## Stacked RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUGH9dDasFr2"
      },
      "source": [
        "![alt](img/stacked.webp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrUxBJv6sFr2",
        "outputId": "317c0a32-9cbf-4fc5-9591-6adb15ca1786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# parameters for data load\n",
        "num_words = 30000\n",
        "maxlen = 50\n",
        "test_split = 0.3\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words = num_words, maxlen = maxlen, test_split = test_split)\n",
        "\n",
        "# pad the sequences with zeros \n",
        "# padding parameter is set to 'post' => 0's are appended to end of sequences\n",
        "X_train = pad_sequences(X_train, padding = 'post')\n",
        "X_test = pad_sequences(X_test, padding = 'post')\n",
        "\n",
        "X_train = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "y_data = np.concatenate((y_train, y_test))\n",
        "y_data = to_categorical(y_data)\n",
        "y_train = y_data[:1395]\n",
        "y_test = y_data[1395:]\n",
        "\n",
        "def stacked_vanilla_rnn():\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(50, input_shape = (49,1), return_sequences = True))   # return_sequences parameter has to be set True to stack\n",
        "    model.add(SimpleRNN(50, return_sequences = False))\n",
        "    model.add(Dense(46))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.Adam(lr = 0.001)\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn = stacked_vanilla_rnn, epochs = 200, batch_size = 50, verbose = 1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_test_ = np.argmax(y_test, axis = 1)\n",
        "\n",
        "print(accuracy_score(y_pred, y_test_))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "28/28 [==============================] - 2s 17ms/step - loss: 2.7998 - accuracy: 0.4300\n",
            "Epoch 2/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.2804 - accuracy: 0.7173\n",
            "Epoch 3/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.2085 - accuracy: 0.6997\n",
            "Epoch 4/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.1621 - accuracy: 0.7117\n",
            "Epoch 5/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.1818 - accuracy: 0.7137\n",
            "Epoch 6/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 1.1609 - accuracy: 0.7054\n",
            "Epoch 7/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 1.1355 - accuracy: 0.7315\n",
            "Epoch 8/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.1807 - accuracy: 0.7118\n",
            "Epoch 9/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 1.2171 - accuracy: 0.6996\n",
            "Epoch 10/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.1636 - accuracy: 0.7165\n",
            "Epoch 11/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.1454 - accuracy: 0.7201\n",
            "Epoch 12/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.1707 - accuracy: 0.7027\n",
            "Epoch 13/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 1.0917 - accuracy: 0.7320\n",
            "Epoch 14/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.1638 - accuracy: 0.7151\n",
            "Epoch 15/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.0858 - accuracy: 0.7117\n",
            "Epoch 16/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 1.1500 - accuracy: 0.7173\n",
            "Epoch 17/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.1323 - accuracy: 0.7162\n",
            "Epoch 18/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.1903 - accuracy: 0.6935\n",
            "Epoch 19/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.1617 - accuracy: 0.7110\n",
            "Epoch 20/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 1.1078 - accuracy: 0.7254\n",
            "Epoch 21/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.1445 - accuracy: 0.7031\n",
            "Epoch 22/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 1.1827 - accuracy: 0.7068\n",
            "Epoch 23/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 1.1898 - accuracy: 0.7014\n",
            "Epoch 24/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.1445 - accuracy: 0.6938\n",
            "Epoch 25/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.0185 - accuracy: 0.7272\n",
            "Epoch 26/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 1.0599 - accuracy: 0.7232\n",
            "Epoch 27/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.0553 - accuracy: 0.7231\n",
            "Epoch 28/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 1.0378 - accuracy: 0.7311\n",
            "Epoch 29/200\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 1.0373 - accuracy: 0.7254\n",
            "Epoch 30/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.0124 - accuracy: 0.7321\n",
            "Epoch 31/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.0114 - accuracy: 0.7388\n",
            "Epoch 32/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 1.0172 - accuracy: 0.7335\n",
            "Epoch 33/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 1.0309 - accuracy: 0.7193\n",
            "Epoch 34/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 1.0009 - accuracy: 0.7285\n",
            "Epoch 35/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 1.0305 - accuracy: 0.7163\n",
            "Epoch 36/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.9496 - accuracy: 0.7329\n",
            "Epoch 37/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 1.0051 - accuracy: 0.7342\n",
            "Epoch 38/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.9172 - accuracy: 0.7508\n",
            "Epoch 39/200\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.9483 - accuracy: 0.7331\n",
            "Epoch 40/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.8965 - accuracy: 0.7482\n",
            "Epoch 41/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.9351 - accuracy: 0.7378\n",
            "Epoch 42/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.9277 - accuracy: 0.7450\n",
            "Epoch 43/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.9583 - accuracy: 0.7343\n",
            "Epoch 44/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.9431 - accuracy: 0.7251\n",
            "Epoch 45/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.9282 - accuracy: 0.7353\n",
            "Epoch 46/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8987 - accuracy: 0.7536\n",
            "Epoch 47/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.9528 - accuracy: 0.7289\n",
            "Epoch 48/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.9614 - accuracy: 0.7158\n",
            "Epoch 49/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.9196 - accuracy: 0.7297\n",
            "Epoch 50/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8629 - accuracy: 0.7492\n",
            "Epoch 51/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.9575 - accuracy: 0.7260\n",
            "Epoch 52/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.9125 - accuracy: 0.7401\n",
            "Epoch 53/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.8632 - accuracy: 0.7527\n",
            "Epoch 54/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.9061 - accuracy: 0.7451\n",
            "Epoch 55/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.8935 - accuracy: 0.7291\n",
            "Epoch 56/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8005 - accuracy: 0.7632\n",
            "Epoch 57/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.8575 - accuracy: 0.7522\n",
            "Epoch 58/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8724 - accuracy: 0.7347\n",
            "Epoch 59/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.8483 - accuracy: 0.7579\n",
            "Epoch 60/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.8444 - accuracy: 0.7466\n",
            "Epoch 61/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.8582 - accuracy: 0.7346\n",
            "Epoch 62/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.8031 - accuracy: 0.7682\n",
            "Epoch 63/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.8445 - accuracy: 0.7461\n",
            "Epoch 64/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8200 - accuracy: 0.7462\n",
            "Epoch 65/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.8448 - accuracy: 0.7468\n",
            "Epoch 66/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8453 - accuracy: 0.7401\n",
            "Epoch 67/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.7956 - accuracy: 0.7581\n",
            "Epoch 68/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.8215 - accuracy: 0.7421\n",
            "Epoch 69/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.7960 - accuracy: 0.7556\n",
            "Epoch 70/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.7649 - accuracy: 0.7580\n",
            "Epoch 71/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.8347 - accuracy: 0.7319\n",
            "Epoch 72/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.8177 - accuracy: 0.7392\n",
            "Epoch 73/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8019 - accuracy: 0.7538\n",
            "Epoch 74/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.7806 - accuracy: 0.7494\n",
            "Epoch 75/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.7630 - accuracy: 0.7737\n",
            "Epoch 76/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8003 - accuracy: 0.7562\n",
            "Epoch 77/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.7215 - accuracy: 0.7742\n",
            "Epoch 78/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.7632 - accuracy: 0.7702\n",
            "Epoch 79/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.7420 - accuracy: 0.7621\n",
            "Epoch 80/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.7448 - accuracy: 0.7578\n",
            "Epoch 81/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.7460 - accuracy: 0.7615\n",
            "Epoch 82/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.7437 - accuracy: 0.7655\n",
            "Epoch 83/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.7574 - accuracy: 0.7729\n",
            "Epoch 84/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.7514 - accuracy: 0.7551\n",
            "Epoch 85/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.8062 - accuracy: 0.7599\n",
            "Epoch 86/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.7527 - accuracy: 0.7565\n",
            "Epoch 87/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.8100 - accuracy: 0.7530\n",
            "Epoch 88/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 0.7590 - accuracy: 0.7608\n",
            "Epoch 89/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.7450 - accuracy: 0.7548\n",
            "Epoch 90/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.7562 - accuracy: 0.7690\n",
            "Epoch 91/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.7459 - accuracy: 0.7685\n",
            "Epoch 92/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.7340 - accuracy: 0.7682\n",
            "Epoch 93/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.7387 - accuracy: 0.7591\n",
            "Epoch 94/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.7365 - accuracy: 0.7689\n",
            "Epoch 95/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.7752 - accuracy: 0.7464\n",
            "Epoch 96/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.7714 - accuracy: 0.7522\n",
            "Epoch 97/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.7419 - accuracy: 0.7492\n",
            "Epoch 98/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.7184 - accuracy: 0.7581\n",
            "Epoch 99/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.7215 - accuracy: 0.7645\n",
            "Epoch 100/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.6852 - accuracy: 0.7792\n",
            "Epoch 101/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.7049 - accuracy: 0.7817\n",
            "Epoch 102/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.7440 - accuracy: 0.7609\n",
            "Epoch 103/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6679 - accuracy: 0.7813\n",
            "Epoch 104/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.6602 - accuracy: 0.7871\n",
            "Epoch 105/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.7259 - accuracy: 0.7636\n",
            "Epoch 106/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.6539 - accuracy: 0.7857\n",
            "Epoch 107/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6996 - accuracy: 0.7701\n",
            "Epoch 108/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.6851 - accuracy: 0.7790\n",
            "Epoch 109/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.6895 - accuracy: 0.7763\n",
            "Epoch 110/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.6956 - accuracy: 0.7685\n",
            "Epoch 111/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.6728 - accuracy: 0.7870\n",
            "Epoch 112/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 0.6334 - accuracy: 0.7904\n",
            "Epoch 113/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6402 - accuracy: 0.7824\n",
            "Epoch 114/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.6600 - accuracy: 0.7795\n",
            "Epoch 115/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6950 - accuracy: 0.7639\n",
            "Epoch 116/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.6413 - accuracy: 0.7780\n",
            "Epoch 117/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.6488 - accuracy: 0.7827\n",
            "Epoch 118/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.6053 - accuracy: 0.8051\n",
            "Epoch 119/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6384 - accuracy: 0.7902\n",
            "Epoch 120/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 0.6243 - accuracy: 0.7886\n",
            "Epoch 121/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.6402 - accuracy: 0.7863\n",
            "Epoch 122/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.6202 - accuracy: 0.7886\n",
            "Epoch 123/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6253 - accuracy: 0.7861\n",
            "Epoch 124/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6536 - accuracy: 0.7766\n",
            "Epoch 125/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.5745 - accuracy: 0.8001\n",
            "Epoch 126/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6288 - accuracy: 0.7969\n",
            "Epoch 127/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6128 - accuracy: 0.7964\n",
            "Epoch 128/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.5937 - accuracy: 0.7959\n",
            "Epoch 129/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5817 - accuracy: 0.8099\n",
            "Epoch 130/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.5893 - accuracy: 0.7937\n",
            "Epoch 131/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5831 - accuracy: 0.7958\n",
            "Epoch 132/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.6327 - accuracy: 0.7858\n",
            "Epoch 133/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 0.6094 - accuracy: 0.7985\n",
            "Epoch 134/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.5716 - accuracy: 0.8024\n",
            "Epoch 135/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.5771 - accuracy: 0.8021\n",
            "Epoch 136/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.5832 - accuracy: 0.7924\n",
            "Epoch 137/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5835 - accuracy: 0.7968\n",
            "Epoch 138/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5509 - accuracy: 0.8251\n",
            "Epoch 139/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5709 - accuracy: 0.8076\n",
            "Epoch 140/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.5292 - accuracy: 0.8155\n",
            "Epoch 141/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5758 - accuracy: 0.7997\n",
            "Epoch 142/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5448 - accuracy: 0.8002\n",
            "Epoch 143/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.5367 - accuracy: 0.8155\n",
            "Epoch 144/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.5628 - accuracy: 0.8036\n",
            "Epoch 145/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.5498 - accuracy: 0.8184\n",
            "Epoch 146/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.5440 - accuracy: 0.8092\n",
            "Epoch 147/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6287 - accuracy: 0.7841\n",
            "Epoch 148/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.5410 - accuracy: 0.8163\n",
            "Epoch 149/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.5458 - accuracy: 0.8157\n",
            "Epoch 150/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5069 - accuracy: 0.8234\n",
            "Epoch 151/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.5431 - accuracy: 0.8174\n",
            "Epoch 152/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5189 - accuracy: 0.8226\n",
            "Epoch 153/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.5439 - accuracy: 0.8118\n",
            "Epoch 154/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5398 - accuracy: 0.8166\n",
            "Epoch 155/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5260 - accuracy: 0.8206\n",
            "Epoch 156/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.5717 - accuracy: 0.7958\n",
            "Epoch 157/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.6197 - accuracy: 0.7815\n",
            "Epoch 158/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5199 - accuracy: 0.8224\n",
            "Epoch 159/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.5040 - accuracy: 0.8169\n",
            "Epoch 160/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5198 - accuracy: 0.8297\n",
            "Epoch 161/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5037 - accuracy: 0.8175\n",
            "Epoch 162/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.5403 - accuracy: 0.8177\n",
            "Epoch 163/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4964 - accuracy: 0.8206\n",
            "Epoch 164/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4942 - accuracy: 0.8143\n",
            "Epoch 165/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4796 - accuracy: 0.8304\n",
            "Epoch 166/200\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.4626 - accuracy: 0.8474\n",
            "Epoch 167/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5276 - accuracy: 0.8017\n",
            "Epoch 168/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.5014 - accuracy: 0.8156\n",
            "Epoch 169/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.5168 - accuracy: 0.8135\n",
            "Epoch 170/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4622 - accuracy: 0.8420\n",
            "Epoch 171/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4737 - accuracy: 0.8369\n",
            "Epoch 172/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4891 - accuracy: 0.8344\n",
            "Epoch 173/200\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.4372 - accuracy: 0.8491\n",
            "Epoch 174/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.4658 - accuracy: 0.8419\n",
            "Epoch 175/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4475 - accuracy: 0.8373\n",
            "Epoch 176/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 0.4537 - accuracy: 0.8392\n",
            "Epoch 177/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4443 - accuracy: 0.8440\n",
            "Epoch 178/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4409 - accuracy: 0.8538\n",
            "Epoch 179/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4254 - accuracy: 0.8559\n",
            "Epoch 180/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4345 - accuracy: 0.8496\n",
            "Epoch 181/200\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.4327 - accuracy: 0.8538\n",
            "Epoch 182/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.4227 - accuracy: 0.8587\n",
            "Epoch 183/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4437 - accuracy: 0.8436\n",
            "Epoch 184/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.4880 - accuracy: 0.8437\n",
            "Epoch 185/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4543 - accuracy: 0.8403\n",
            "Epoch 186/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.4769 - accuracy: 0.8323\n",
            "Epoch 187/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.4200 - accuracy: 0.8522\n",
            "Epoch 188/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4724 - accuracy: 0.8413\n",
            "Epoch 189/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.3946 - accuracy: 0.8669\n",
            "Epoch 190/200\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.4365 - accuracy: 0.8485\n",
            "Epoch 191/200\n",
            "28/28 [==============================] - 0s 15ms/step - loss: 0.4048 - accuracy: 0.8573\n",
            "Epoch 192/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.3789 - accuracy: 0.8713\n",
            "Epoch 193/200\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.4086 - accuracy: 0.8654\n",
            "Epoch 194/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.4192 - accuracy: 0.8558\n",
            "Epoch 195/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.3895 - accuracy: 0.8820\n",
            "Epoch 196/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4591 - accuracy: 0.8438\n",
            "Epoch 197/200\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.4541 - accuracy: 0.8379\n",
            "Epoch 198/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4220 - accuracy: 0.8524\n",
            "Epoch 199/200\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.4319 - accuracy: 0.8487\n",
            "Epoch 200/200\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.3963 - accuracy: 0.8614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 6ms/step\n",
            "0.7512520868113522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9KSNoQpsFr4"
      },
      "source": [
        "### What is Long Short Term Memory (LSTM)?\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks are a modified version of recurrent neural networks, which makes it easier to remember past data in memory. The vanishing gradient problem of RNN is resolved here. LSTM is well-suited to classify, process and predict time series given time lags of unknown duration. It trains the model by using back-propagation. In an LSTM network, three gates are present:\n",
        "\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/1*MwU5yk8f9d6IcLybvGgNxA.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnPUA7uusFr4"
      },
      "source": [
        "#### Forget gate\n",
        "\n",
        "It discover what details to be discarded from the block. It is decided by the sigmoid function. it looks at the previous state(ht-1) and the content input(Xt) and outputs a number between 0(omit this)and 1(keep this)for each number in the cell state Ct−1.\n",
        "\n",
        "![alt](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
        "\n",
        "\n",
        "EX: lets say ht-1 → Rishab and Rahul plays well in basket ball.\n",
        "\n",
        "Xt →Rahul is really good at webdesigning.\n",
        "\n",
        "*    Forget gate realizes that there might be change in the context after encounter its first fullstop.\n",
        "*    Compare with Current Input Xt.\n",
        "*    Its important to know that next sentence, talks about Rahul. so information about Rishab is omitted.\n",
        "\n",
        "\n",
        "\n",
        "#### Input Gate\n",
        "\n",
        "Input gate — discover which value from input should be used to modify the memory. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1.\n",
        "\n",
        "![alt](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
        "![alt](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
        "\n",
        "EX: Rahul good webdesigining, yesterday he told me that he is a university topper.\n",
        "\n",
        "*    input gate analysis the important information.\n",
        "*    Rahul good webdesigining, he is university topper is important.\n",
        "*    yesterday he told me that is not important, hence forgotten.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f9LV3FHsFr5"
      },
      "source": [
        "#### Output Gate\n",
        "\n",
        "Output gate — the input and the memory of the block is used to decide the output. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1 and multiplied with output of Sigmoid.\n",
        "\n",
        "![alt](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
        "\n",
        "EX: Rahul good webdesigining, he is university topper so the Merit student _______________ was awarded University Gold medalist.\n",
        "\n",
        "* there could be lot of choices for the empty dash. this final gate replaces it with Rahul."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhbFTlz8sFr5"
      },
      "source": [
        "## All together LSTM\n",
        "\n",
        "![alt](https://miro.medium.com/proxy/1*goJVQs-p9kgLODFNyhl9zA.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dORGfAVsFr6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}